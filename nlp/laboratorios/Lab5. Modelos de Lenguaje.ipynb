{
 "metadata": {
  "name": "",
  "signature": "sha256:1abb376a2e544f9d156e264e7bb5832c75c36e75e713c55afda3d345545d91cb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lab 5. Modelos de Lenguaje para Espa\u00f1ol\n",
      "===============================================\n",
      "Data a utilizar: Noticias Espa\u00f1olas (CESS-ESP Treebank)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "from sklearn.cross_validation import train_test_split\n",
      "import ipdb"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Variables globales"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "STOP = '</s>' # simbolo de fin de oracion\n",
      "RARE = '<R>'  # etiqueta para palabra de baja frecuencia\n",
      "THR = 5       # umbral de baja frecuencia (<=THR -> RARE)\n",
      "\n",
      "stemmer = SnowballStemmer('spanish') # Stemmer para espa\u00f1ol"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Funciones de Modelo\n",
      "---------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# funci\u00f3n de preprocesado de texto:\n",
      "#   normaliza palabras y extrae unigramas y bigramas\n",
      "def preprocess_corpus(corpus,vocab,stemming=True):\n",
      "    \"\"\"\n",
      "    corpus: [ [word] ] lista de oraciones. Oracion: lista de palabras\n",
      "    vocab: {word:freq} diccionario de palabra : frecuencia\n",
      "    returns: corpus_preprocesado, unigramas, bigramas\n",
      "    \"\"\"\n",
      "    unigrams = []\n",
      "    bigrams = []\n",
      "    new_corpus = []\n",
      "    for sent in corpus:\n",
      "        new_sent = []\n",
      "        prev_word = sent[0]\n",
      "        for i,word in enumerate(sent):\n",
      "            new_word = ''\n",
      "            ####\n",
      "            if word in vocab:\n",
      "                if vocab[word]>THR:\n",
      "                    if stemming:\n",
      "                        new_word = stemmer.stem(word)\n",
      "                    else:\n",
      "                        new_word = word\n",
      "                else:\n",
      "                    new_word = RARE\n",
      "            else:\n",
      "                new_word = RARE\n",
      "            ####\n",
      "            new_sent.append(new_word)\n",
      "            # hallando bigramas\n",
      "            ####\n",
      "            if i>0:\n",
      "                bigrams.append( (prev_word,new_word) )\n",
      "            ####\n",
      "            prev_word = new_word\n",
      "        # hallando unigramas\n",
      "        ####\n",
      "        unigrams += new_sent\n",
      "        #if 'armada' in new_sent:\n",
      "        #    ipdb.set_trace()\n",
      "        # agregar terminos STOP\n",
      "        ####\n",
      "        unigrams.append(STOP)\n",
      "        bigrams.append( (new_sent[-1],STOP) )\n",
      "        ####\n",
      "        new_corpus.append(new_sent)\n",
      "        \n",
      "    return new_corpus,unigrams,bigrams"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# C\u00e1lculo de probabilidades P(wi) y P(wi|w_{i-1})\n",
      "def language_model(unigrams,bigrams):\n",
      "    \"\"\"\n",
      "    unigrams: lista de unigramas\n",
      "    bigrams: liste de bigramas [pares (w_{i-1},w_i)]\n",
      "    return: P(wi), P(wi|w_{i-1})\n",
      "    \"\"\"\n",
      "    #ipdb.set_trace()\n",
      "    \n",
      "    freq_ug = nltk.FreqDist(unigrams)\n",
      "    N_ug = len(unigrams)\n",
      "    prob_unigrams = { word:freq/N_ug for word,freq in freq_ug.items() }\n",
      "    \n",
      "    freq_bg = nltk.ConditionalFreqDist(bigrams)\n",
      "    cprob_bigrams = {}\n",
      "    \n",
      "    for w_prev,_ in freq_bg.items():\n",
      "        prob_wp = {}\n",
      "        for w_curr in freq_ug.keys():\n",
      "            if w_curr in freq_bg[w_prev]:\n",
      "                prob_wp[w_curr] = freq_bg[w_prev][w_curr] / freq_ug[w_prev]\n",
      "            else:\n",
      "                prob_wp[w_curr] = 0\n",
      "        cprob_bigrams[w_prev] = prob_wp\n",
      "        \n",
      "    return prob_unigrams,cprob_bigrams"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# C\u00e1lculo de probabilidad logar\u00edtmica de una oraci\u00f3n\n",
      "def log_prob_sentence_bigram(sent,lm_ug,lm_bg):\n",
      "    \"\"\"\n",
      "    sent: oracion con 'm' palabras. [word]\n",
      "    lm_ug: probabilidades de unigramas {wi: P(wi)}\n",
      "    lm_bg: probabilidades de bigramas {w_{i-1}: P(wi|w_{i-1}}\n",
      "    return: log(P(w0)) + Sum_{i=1,m}( log(P(wi|w_{i-1})) ) + log(P(STOP|w_m))\n",
      "    \"\"\"\n",
      "    log_prob = np.log( lm_ug[sent[0]] + 1e-10)\n",
      "    ####\n",
      "    for i in range(1,len(sent)):\n",
      "        wi = sent[i]\n",
      "        wi_1 = sent[i-1]\n",
      "        log_prob += np.log( lm_bg[wi_1][wi] +1e-10)\n",
      "    ####\n",
      "    log_prob += np.log( lm_bg[sent[-1]][STOP] + 1e-10)\n",
      "               \n",
      "    return log_prob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def log_prob_sentence(sent,lm_ug):\n",
      "    log_prob = 0\n",
      "    ###\n",
      "    for token in sent:\n",
      "        log_prob += np.log( lm_ug[token] +1e-10)\n",
      "    ###\n",
      "    return log_prob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# C\u00e1lculo de Perplexity para Modelo de Bigramas\n",
      "def perplexity_bigram(corpus,lm_ug,lm_bg):\n",
      "    L = 0\n",
      "    N_corpus = 0\n",
      "    for sent in corpus:\n",
      "        L += log_prob_sentence_bigram(sent,lm_ug,lm_bg)\n",
      "        N_corpus += len(sent)+1\n",
      "    \n",
      "    return np.power(2,-L/N_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# C\u00e1lculo de Perplexity para Modelo de Unigramas\n",
      "def perplexity_unigram(corpus,lm_ug):\n",
      "    total_unigrams = 1\n",
      "    L = 0\n",
      "    for sent in corpus:\n",
      "        L += log_prob_sentence(sent,lm_ug)\n",
      "        total_unigrams += len(sent)\n",
      "        \n",
      "        \n",
      "    return np.power(2,-L/total_unigrams)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "==========================================================================================================\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lectura y separaci\u00f3n de data\n",
      "--------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Lectura del corpus\n",
      "len_corpus = 6030\n",
      "corpus = nltk.corpus.cess_esp.sents()[:len_corpus]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Dividiendo en training y testing\n",
      "test_perc = 0.4\n",
      "train_ids,test_ids = train_test_split(range(len_corpus),test_size=test_perc,random_state=42)\n",
      "\n",
      "train_corpus,test_corpus = [],[]\n",
      "for _id in train_ids:\n",
      "    train_corpus.append(corpus[_id])\n",
      "for _id in test_ids:\n",
      "    test_corpus.append(corpus[_id])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Training size:\",len(train_corpus))\n",
      "print(\"Testing size:\",len(test_corpus))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training size: 3618\n",
        "Testing size: 2412\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Obteniendo vocabulario\n",
      "-----------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# obteniedo vocabulario de data de entrenamiento\n",
      "train_words = []\n",
      "for sent in train_corpus:\n",
      "    # pasando a minuscula\n",
      "\ttrain_words.extend([word.lower() for word in sent])\n",
      "\n",
      "train_vocab = nltk.FreqDist(train_words)\n",
      "del train_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Preprocesando data\n",
      "--------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemming = False # hacer stemming?\n",
      "\n",
      "# Preprocesar data de entrenamiento\n",
      "train,U,B = preprocess_corpus(train_corpus,train_vocab,stemming=stemming)\n",
      "# Preprocesar data de testeo\n",
      "test,_,_ = preprocess_corpus(test_corpus,train_vocab,stemming=stemming)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Entrenando modelo\n",
      "--------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Ajustar modelo en training data\n",
      "lm_ug,lm_bg = language_model(U,B)\n",
      "del U\n",
      "del B\n",
      "\n",
      "# Prueba\n",
      "PP= perplexity_unigram(train,lm_ug)\n",
      "P = perplexity_bigram(train,lm_ug,lm_bg)\n",
      "print(\"PP training set[UNIGRAM]: %.2f\" % PP)\n",
      "print(\"PP training set[BIGRAM]: %.2f\" % P)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "PP training set[UNIGRAM]: 21.03\n",
        "PP training set[BIGRAM]: 8.57\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Testeando\n",
      "--------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Prueba en Testing data\n",
      "PP_test = perplexity_unigram(test,lm_ug)\n",
      "P_test = perplexity_bigram(test,lm_ug,lm_bg)\n",
      "print(\"Test[UNIGRAM]: %.2f\" % PP_test)\n",
      "print(\"Test[BIGRAM]: %.2f\" % P_test)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyError",
       "evalue": "'armada'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-15-214fd44db21f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Prueba en Testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mPP_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperplexity_unigram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlm_ug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mP_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperplexity_bigram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlm_ug\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlm_bg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test[UNIGRAM]: %.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mPP_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test[BIGRAM]: %.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mP_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-8-1d6b1d2005f1>\u001b[0m in \u001b[0;36mperplexity_unigram\u001b[0;34m(corpus, lm_ug)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mL\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlog_prob_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlm_ug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtotal_unigrams\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-6-7ee3f9fe2474>\u001b[0m in \u001b[0;36mlog_prob_sentence\u001b[0;34m(sent, lm_ug)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlm_ug\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyError\u001b[0m: 'armada'"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_vocab['armada']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}